# -*- coding: utf-8 -*-
"""SENTIMENT ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12BWWL38kBFz0xFVZyEzvP6KillRYUjWN

Sentiment analysis
input will have text data
output will have 0 or 1 as positive or negative sentiment

we will form the vocab, each word is provided a vector
size different
we do padding
first we convert text to vectors
"""

import numpy as np

docs = ['go india',
		'india india',
		'hip hip hurray',
		'jeetega bhai jeetega india jeetega',
		'bharat mata ki jai',
		'kohli kohli',
		'sachin sachin',
		'dhoni dhoni',
		'modi ji ki jai',
		'inquilab zindabad']

from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(oov_token='<nothing>')

!pip install tensorflow

tokenizer.fit_on_texts(docs)

len(tokenizer.word_index)

tokenizer.word_counts

sequences = tokenizer.texts_to_sequences(docs)
sequences

from keras.utils import pad_sequences

sequences = pad_sequences(sequences, padding = 'post')

sequences

model = Sequential()
model.add(Embedding(19, output_dim=2, input_length=5))

model.summary

model.compile('adam', 'accuracy')

pred= model.predict(sequences)
print(pred)
#each word is represented by 1x2 vector, even zeroes have their vectors

#inmd sentiment classfication dataset
from keras.datasets import imdb
from keras import Sequential
from keras.layers import Dense, SimpleRNN, Embedding, Flatten

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)

X_train.shape

X_test.shape

X_train = pad_sequences(X_train, padding = 'post', maxlen=50)
X_test = pad_sequences(X_test, padding = 'post', maxlen=50)

X_train.shape

X_train[0]

# model = Sequential()
# model.add(SimpleRNN(32, input_shape = (50, 1), return_sequences=False))
# model.add(Dense(1, activation = 'sigmoid'))

# model.summary

#10000, 2, 50
# here 10000 is the vocabulary, 2 is the dimension of the output, and 50 is the input size
model = Sequential()
model.add(Embedding(10000, 2, input_length=50))
model.add(SimpleRNN(32, return_sequences=False))
model.add(Dense(1, activation = 'sigmoid'))

"""50 time steps will be taking place
total weights=1088 in the simple rnn
total weights =33 in dense rnn

return sequences are kept false, becoz the output at the end of each time step is fed into the next time step
Ot-1 is used for Ot for finding new prediction value

"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))